layer {
  name: "input"
  type: "Input"
  top: "input"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 320
      dim: 320
    }
  }
}
layer {
  name: "Conv_0"
  type: "Convolution"
  bottom: "input"
  top: "299"
  convolution_param {
    num_output: 12
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_1"
  type: "ReLU"
  bottom: "299"
  top: "300"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_2"
  type: "Convolution"
  bottom: "300"
  top: "301"
  convolution_param {
    num_output: 8
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_3"
  type: "ReLU"
  bottom: "301"
  top: "302"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_4"
  type: "Convolution"
  bottom: "302"
  top: "303"
  convolution_param {
    num_output: 16
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_5"
  type: "ReLU"
  bottom: "303"
  top: "304"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_6"
  type: "Convolution"
  bottom: "304"
  top: "305"
  convolution_param {
    num_output: 8
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_7"
  type: "ReLU"
  bottom: "305"
  top: "306"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_8"
  type: "Convolution"
  bottom: "306"
  top: "307"
  convolution_param {
    num_output: 16
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_9"
  type: "ReLU"
  bottom: "307"
  top: "308"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Add_10"
  type: "Eltwise"
  bottom: "304"
  bottom: "308"
  top: "309"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_11"
  type: "Convolution"
  bottom: "309"
  top: "310"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_12"
  type: "ReLU"
  bottom: "310"
  top: "311"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_13"
  type: "Convolution"
  bottom: "311"
  top: "312"
  convolution_param {
    num_output: 16
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_14"
  type: "ReLU"
  bottom: "312"
  top: "313"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_15"
  type: "Convolution"
  bottom: "313"
  top: "314"
  convolution_param {
    num_output: 16
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_16"
  type: "ReLU"
  bottom: "314"
  top: "315"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_17"
  type: "Convolution"
  bottom: "315"
  top: "316"
  convolution_param {
    num_output: 16
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_18"
  type: "ReLU"
  bottom: "316"
  top: "317"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Add_19"
  type: "Eltwise"
  bottom: "313"
  bottom: "317"
  top: "318"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_20"
  type: "Convolution"
  bottom: "318"
  top: "319"
  convolution_param {
    num_output: 16
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_21"
  type: "ReLU"
  bottom: "319"
  top: "320"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_22"
  type: "Convolution"
  bottom: "320"
  top: "321"
  convolution_param {
    num_output: 16
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_23"
  type: "ReLU"
  bottom: "321"
  top: "322"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Add_24"
  type: "Eltwise"
  bottom: "318"
  bottom: "322"
  top: "323"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_25"
  type: "Convolution"
  bottom: "323"
  top: "324"
  convolution_param {
    num_output: 16
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_26"
  type: "ReLU"
  bottom: "324"
  top: "325"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_27"
  type: "Convolution"
  bottom: "325"
  top: "326"
  convolution_param {
    num_output: 16
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_28"
  type: "ReLU"
  bottom: "326"
  top: "327"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Add_29"
  type: "Eltwise"
  bottom: "323"
  bottom: "327"
  top: "328"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_30"
  type: "Convolution"
  bottom: "328"
  top: "329"
  convolution_param {
    num_output: 16
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Conv_31"
  type: "Convolution"
  bottom: "311"
  top: "330"
  convolution_param {
    num_output: 16
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Concat_32"
  type: "Concat"
  bottom: "329"
  bottom: "330"
  top: "331"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNormalization_33_bn"
  type: "BatchNorm"
  bottom: "331"
  top: "332"
  batch_norm_param {
    use_global_stats: true
    eps: 9.999999747378752e-05
  }
}
layer {
  name: "BatchNormalization_33"
  type: "Scale"
  bottom: "332"
  top: "332"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "LeakyRelu_34"
  type: "ReLU"
  bottom: "332"
  top: "333"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_35"
  type: "Convolution"
  bottom: "333"
  top: "334"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_36"
  type: "ReLU"
  bottom: "334"
  top: "335"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_37"
  type: "Convolution"
  bottom: "335"
  top: "336"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_38"
  type: "ReLU"
  bottom: "336"
  top: "337"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_39"
  type: "Convolution"
  bottom: "337"
  top: "338"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_40"
  type: "ReLU"
  bottom: "338"
  top: "339"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_41"
  type: "Convolution"
  bottom: "339"
  top: "340"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_42"
  type: "ReLU"
  bottom: "340"
  top: "341"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_43"
  type: "Convolution"
  bottom: "341"
  top: "342"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_44"
  type: "ReLU"
  bottom: "342"
  top: "343"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Add_45"
  type: "Eltwise"
  bottom: "339"
  bottom: "343"
  top: "344"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_46"
  type: "Convolution"
  bottom: "344"
  top: "345"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_47"
  type: "ReLU"
  bottom: "345"
  top: "346"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_48"
  type: "Convolution"
  bottom: "346"
  top: "347"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_49"
  type: "ReLU"
  bottom: "347"
  top: "348"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Add_50"
  type: "Eltwise"
  bottom: "344"
  bottom: "348"
  top: "349"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_51"
  type: "Convolution"
  bottom: "349"
  top: "350"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_52"
  type: "ReLU"
  bottom: "350"
  top: "351"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_53"
  type: "Convolution"
  bottom: "351"
  top: "352"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_54"
  type: "ReLU"
  bottom: "352"
  top: "353"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Add_55"
  type: "Eltwise"
  bottom: "349"
  bottom: "353"
  top: "354"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_56"
  type: "Convolution"
  bottom: "354"
  top: "355"
  convolution_param {
    num_output: 32
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Conv_57"
  type: "Convolution"
  bottom: "337"
  top: "356"
  convolution_param {
    num_output: 32
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Concat_58"
  type: "Concat"
  bottom: "355"
  bottom: "356"
  top: "357"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNormalization_59_bn"
  type: "BatchNorm"
  bottom: "357"
  top: "358"
  batch_norm_param {
    use_global_stats: true
    eps: 9.999999747378752e-05
  }
}
layer {
  name: "BatchNormalization_59"
  type: "Scale"
  bottom: "358"
  top: "358"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "LeakyRelu_60"
  type: "ReLU"
  bottom: "358"
  top: "359"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_61"
  type: "Convolution"
  bottom: "359"
  top: "360"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_62"
  type: "ReLU"
  bottom: "360"
  top: "361"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_63"
  type: "Convolution"
  bottom: "361"
  top: "362"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_64"
  type: "ReLU"
  bottom: "362"
  top: "363"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_65"
  type: "Convolution"
  bottom: "363"
  top: "364"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_66"
  type: "ReLU"
  bottom: "364"
  top: "365"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "MaxPool_67"
  type: "Pooling"
  bottom: "365"
  top: "366"
  pooling_param {
    pool: MAX
    kernel_h: 5
    kernel_w: 5
    stride_h: 1
    stride_w: 1
    pad_h: 2
    pad_w: 2
  }
}
layer {
  name: "MaxPool_68"
  type: "Pooling"
  bottom: "365"
  top: "367"
  pooling_param {
    pool: MAX
    kernel_h: 9
    kernel_w: 9
    stride_h: 1
    stride_w: 1
    pad_h: 4
    pad_w: 4
  }
}
layer {
  name: "MaxPool_69"
  type: "Pooling"
  bottom: "365"
  top: "368"
  pooling_param {
    pool: MAX
    kernel_h: 13
    kernel_w: 13
    stride_h: 1
    stride_w: 1
    pad_h: 6
    pad_w: 6
  }
}
layer {
  name: "Concat_70"
  type: "Concat"
  bottom: "365"
  bottom: "366"
  bottom: "367"
  bottom: "368"
  top: "369"
  concat_param {
    axis: 1
  }
}
layer {
  name: "Conv_71"
  type: "Convolution"
  bottom: "369"
  top: "370"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_72"
  type: "ReLU"
  bottom: "370"
  top: "371"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_73"
  type: "Convolution"
  bottom: "371"
  top: "372"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_74"
  type: "ReLU"
  bottom: "372"
  top: "373"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_75"
  type: "Convolution"
  bottom: "373"
  top: "374"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_76"
  type: "ReLU"
  bottom: "374"
  top: "375"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_77"
  type: "Convolution"
  bottom: "375"
  top: "376"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_78"
  type: "ReLU"
  bottom: "376"
  top: "377"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Add_79"
  type: "Eltwise"
  bottom: "373"
  bottom: "377"
  top: "378"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_80"
  type: "Convolution"
  bottom: "378"
  top: "379"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_81"
  type: "ReLU"
  bottom: "379"
  top: "380"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_82"
  type: "Convolution"
  bottom: "380"
  top: "381"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_83"
  type: "ReLU"
  bottom: "381"
  top: "382"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Add_84"
  type: "Eltwise"
  bottom: "378"
  bottom: "382"
  top: "383"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Conv_85"
  type: "Convolution"
  bottom: "383"
  top: "384"
  convolution_param {
    num_output: 64
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Conv_86"
  type: "Convolution"
  bottom: "371"
  top: "385"
  convolution_param {
    num_output: 64
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Concat_87"
  type: "Concat"
  bottom: "384"
  bottom: "385"
  top: "386"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNormalization_88_bn"
  type: "BatchNorm"
  bottom: "386"
  top: "387"
  batch_norm_param {
    use_global_stats: true
    eps: 9.999999747378752e-05
  }
}
layer {
  name: "BatchNormalization_88"
  type: "Scale"
  bottom: "387"
  top: "387"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "LeakyRelu_89"
  type: "ReLU"
  bottom: "387"
  top: "388"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_90"
  type: "Convolution"
  bottom: "388"
  top: "389"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_91"
  type: "ReLU"
  bottom: "389"
  top: "390"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_92"
  type: "Convolution"
  bottom: "390"
  top: "391"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_93"
  type: "ReLU"
  bottom: "391"
  top: "392"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_94"
  type: "Convolution"
  bottom: "392"
  top: "393"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_95"
  type: "ReLU"
  bottom: "393"
  top: "394"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_96"
  type: "Convolution"
  bottom: "394"
  top: "395"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_97"
  type: "ReLU"
  bottom: "395"
  top: "396"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_98"
  type: "Convolution"
  bottom: "396"
  top: "397"
  convolution_param {
    num_output: 64
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Conv_99"
  type: "Convolution"
  bottom: "390"
  top: "398"
  convolution_param {
    num_output: 64
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Concat_100"
  type: "Concat"
  bottom: "397"
  bottom: "398"
  top: "399"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNormalization_101_bn"
  type: "BatchNorm"
  bottom: "399"
  top: "400"
  batch_norm_param {
    use_global_stats: true
    eps: 9.999999747378752e-05
  }
}
layer {
  name: "BatchNormalization_101"
  type: "Scale"
  bottom: "400"
  top: "400"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "LeakyRelu_102"
  type: "ReLU"
  bottom: "400"
  top: "401"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_103"
  type: "Convolution"
  bottom: "401"
  top: "402"
  convolution_param {
    num_output: 128
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_104"
  type: "ReLU"
  bottom: "402"
  top: "403"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_105"
  type: "Convolution"
  bottom: "403"
  top: "404"
  convolution_param {
    num_output: 18
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Resize_134"
  type: "Upsample"
  bottom: "403"
  top: "433"
  upsample_param {
    height: 20
    width: 20
    mode: NEAREST
  }
}
layer {
  name: "Concat_135"
  type: "Concat"
  bottom: "433"
  bottom: "361"
  top: "434"
  concat_param {
    axis: 1
  }
}
layer {
  name: "Conv_136"
  type: "Convolution"
  bottom: "434"
  top: "435"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_137"
  type: "ReLU"
  bottom: "435"
  top: "436"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_138"
  type: "Convolution"
  bottom: "436"
  top: "437"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_139"
  type: "ReLU"
  bottom: "437"
  top: "438"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_140"
  type: "Convolution"
  bottom: "438"
  top: "439"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_141"
  type: "ReLU"
  bottom: "439"
  top: "440"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_142"
  type: "Convolution"
  bottom: "440"
  top: "441"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_143"
  type: "ReLU"
  bottom: "441"
  top: "442"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_144"
  type: "Convolution"
  bottom: "442"
  top: "443"
  convolution_param {
    num_output: 32
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Conv_145"
  type: "Convolution"
  bottom: "436"
  top: "444"
  convolution_param {
    num_output: 32
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Concat_146"
  type: "Concat"
  bottom: "443"
  bottom: "444"
  top: "445"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNormalization_147_bn"
  type: "BatchNorm"
  bottom: "445"
  top: "446"
  batch_norm_param {
    use_global_stats: true
    eps: 9.999999747378752e-05
  }
}
layer {
  name: "BatchNormalization_147"
  type: "Scale"
  bottom: "446"
  top: "446"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "LeakyRelu_148"
  type: "ReLU"
  bottom: "446"
  top: "447"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_149"
  type: "Convolution"
  bottom: "447"
  top: "448"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_150"
  type: "ReLU"
  bottom: "448"
  top: "449"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_151"
  type: "Convolution"
  bottom: "449"
  top: "450"
  convolution_param {
    num_output: 18
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Resize_180"
  type: "Upsample"
  bottom: "449"
  top: "479"
  upsample_param {
    height: 40
    width: 40
    mode: NEAREST
  }
}
layer {
  name: "Concat_181"
  type: "Concat"
  bottom: "479"
  bottom: "335"
  top: "480"
  concat_param {
    axis: 1
  }
}
layer {
  name: "Conv_182"
  type: "Convolution"
  bottom: "480"
  top: "481"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_183"
  type: "ReLU"
  bottom: "481"
  top: "482"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_184"
  type: "Convolution"
  bottom: "482"
  top: "483"
  convolution_param {
    num_output: 16
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_185"
  type: "ReLU"
  bottom: "483"
  top: "484"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_186"
  type: "Convolution"
  bottom: "484"
  top: "485"
  convolution_param {
    num_output: 16
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_187"
  type: "ReLU"
  bottom: "485"
  top: "486"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_188"
  type: "Convolution"
  bottom: "486"
  top: "487"
  convolution_param {
    num_output: 16
    bias_term: true
    group: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_189"
  type: "ReLU"
  bottom: "487"
  top: "488"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_190"
  type: "Convolution"
  bottom: "488"
  top: "489"
  convolution_param {
    num_output: 16
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Conv_191"
  type: "Convolution"
  bottom: "482"
  top: "490"
  convolution_param {
    num_output: 16
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Concat_192"
  type: "Concat"
  bottom: "489"
  bottom: "490"
  top: "491"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNormalization_193_bn"
  type: "BatchNorm"
  bottom: "491"
  top: "492"
  batch_norm_param {
    use_global_stats: true
    eps: 9.999999747378752e-05
  }
}
layer {
  name: "BatchNormalization_193"
  type: "Scale"
  bottom: "492"
  top: "492"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "LeakyRelu_194"
  type: "ReLU"
  bottom: "492"
  top: "493"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_195"
  type: "Convolution"
  bottom: "493"
  top: "494"
  convolution_param {
    num_output: 32
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "LeakyRelu_196"
  type: "ReLU"
  bottom: "494"
  top: "495"
  relu_param {
    negative_slope: 0.10000000149011612
  }
}
layer {
  name: "Conv_197"
  type: "Convolution"
  bottom: "495"
  top: "496"
  convolution_param {
    num_output: 18
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "Reshape_207"
  type: "Reshape"
  bottom: "496"
  top: "510"
  reshape_param {
    shape {
      dim: 3
      dim: 6
      dim: 40
      dim: 40
    }
  }
}
layer {
  name: "Transpose_208"
  type: "Permute"
  bottom: "510"
  top: "511"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "Sigmoid_209"
  type: "Sigmoid"
  bottom: "511"
  top: "output1"
}
layer {
  name: "Reshape_219"
  type: "Reshape"
  bottom: "450"
  top: "526"
  reshape_param {
    shape {
      dim: 3
      dim: 6
      dim: 20
      dim: 20
    }
  }
}
layer {
  name: "Transpose_220"
  type: "Permute"
  bottom: "526"
  top: "527"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "Sigmoid_221"
  type: "Sigmoid"
  bottom: "527"
  top: "output2"
}
layer {
  name: "Reshape_231"
  type: "Reshape"
  bottom: "404"
  top: "542"
  reshape_param {
    shape {
      dim: 3
      dim: 6
      dim: 10
      dim: 10
    }
  }
}
layer {
  name: "Transpose_232"
  type: "Permute"
  bottom: "542"
  top: "543"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "Sigmoid_233"
  type: "Sigmoid"
  bottom: "543"
  top: "output3"
}

